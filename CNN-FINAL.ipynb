{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Modules\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import Counter\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load Data\n",
    "labels = ['contradiction', 'entailment', 'neutral']\n",
    "label_dict = dict(zip(labels, range(0, len(labels))))\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    sent1 = []\n",
    "    sent2 = []\n",
    "    key = []\n",
    "\n",
    "    with open(file_path) as file:\n",
    "        rows = csv.reader(file, delimiter=\"\\t\")\n",
    "        next(rows)  # skips first row\n",
    "        for i,row in enumerate(rows):\n",
    "            sent1.append(row[0].split())\n",
    "            sent2.append(row[1].split())\n",
    "            key.append(row[2].split())\n",
    "            \n",
    "\n",
    "        key = [label for labels in key for label in labels]\n",
    "        target = [float(label_dict[item]) for item in key]\n",
    "\n",
    "        return list(zip(sent1, sent2)), target\n",
    "\n",
    "\n",
    "snli_train = load_data(\"snli_train.tsv\")\n",
    "snli_val = load_data(\"snli_val.tsv\")\n",
    "mnli_train = load_data(\"mnli_train.tsv\")\n",
    "mnli_val = load_data(\"mnli_val.tsv\")\n",
    "\n",
    "# Check Number of Samples\n",
    "print(\"The number of samples in snli_train is {:,d}\".format(len(snli_train[0])))\n",
    "print(\"The number of samples in snli_val is {:,d}\".format(len(snli_val[0])))\n",
    "print(\"The number of samples in mnli_train is {:,d}\".format(len(mnli_train[0])))\n",
    "print(\"The number of samples in mnli_val is {:,d}\".format(len(mnli_val[0])))\n",
    "\n",
    "# Create Data Dictionary\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "\n",
    "# Load Pre-trained Word Vectors\n",
    "def load_embeddings(word2vec, word2id, embedding_dim):\n",
    "    embeddings = np.zeros((len(word2id), embedding_dim))\n",
    "    for word, index in word2id.items():\n",
    "        try:\n",
    "            embeddings[index] = word2vec[word]\n",
    "\n",
    "        except KeyError:\n",
    "            embeddings[index] = np.random.normal(scale=0.6, size=(300,))\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def load_vectors(fname, num_vecs=None):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "\n",
    "        if num_vecs is None:\n",
    "            pass\n",
    "        else:\n",
    "            if len(data) + 1 > num_vecs:\n",
    "                break\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "word_vectors = load_vectors('/Volumes/Samsung USB/Fall 2018/wiki-news-300d-1M.vec',50000)\n",
    "\n",
    "print(\"Total number of words embedded is {:,d}\".format(len(word_vectors)))\n",
    "\n",
    "\n",
    "def data_dictionary(tokens, vocab_size_limit):\n",
    "    token_counter = Counter()\n",
    "    for token in tokens:\n",
    "        token_counter[token] += 1\n",
    "\n",
    "    vocab, count = zip(*token_counter.most_common(vocab_size_limit))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2, 2 + len(vocab))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "\n",
    "token2id, id2token = data_dictionary(list(word_vectors.keys()), 50000)\n",
    "\n",
    "print(\"Total number of words in token2id is {:,d}\".format(len(token2id)))  # Included UNK and PAD index\n",
    "\n",
    "\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "weights=load_embeddings(word_vectors,token2id,300)\n",
    "weights.shape\n",
    "\n",
    "MAX_SENT_LENGTH = max([len(snli_train[0][i][0]) for i in range(len(snli_train[0]))] +\n",
    "                      [len(snli_train[0][i][1]) for i in range(len(snli_train[0]))])\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, word2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list, self.target_list = data\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "        self.word2id = word2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        sent_idx1 = [self.word2id[w] if w in self.word2id.keys() else UNK_IDX for w in\n",
    "                     self.data_list[key][1][:MAX_SENT_LENGTH]]\n",
    "        sent_idx2 = [self.word2id[w] if w in self.word2id.keys() else UNK_IDX for w in\n",
    "                     self.data_list[key][0][:MAX_SENT_LENGTH]]\n",
    "        label = self.target_list[key]\n",
    "        return [sent_idx1, len(sent_idx1), sent_idx2, len(sent_idx2), label]\n",
    "\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sent1_list = []\n",
    "    sent2_list = []\n",
    "\n",
    "    label_list = []\n",
    "    length1_list = []\n",
    "    length2_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])  # Should be 4\n",
    "        length1_list.append(datum[1])\n",
    "        length2_list.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]),\n",
    "                             pad_width=((0, MAX_SENT_LENGTH - datum[1])),\n",
    "                             mode=\"constant\", constant_values=0)\n",
    "        padded_vec2 = np.pad(np.array(datum[2]),\n",
    "                             pad_width=((0, MAX_SENT_LENGTH - datum[3])),\n",
    "                             mode=\"constant\", constant_values=0)\n",
    "\n",
    "        # Addd to list\n",
    "        sent1_list.append(padded_vec1)\n",
    "        sent2_list.append(padded_vec2)\n",
    "\n",
    "    ind_dec_order = np.argsort(length1_list)[::-1]\n",
    "    sent1_list = np.array(sent1_list)[ind_dec_order]\n",
    "    length1_list = np.array(length1_list)[ind_dec_order]\n",
    "    label_list = np.array(label_list)[ind_dec_order]\n",
    "    return [torch.from_numpy(np.array(sent1_list)), torch.LongTensor(length1_list),\n",
    "            torch.from_numpy(np.array(sent2_list)), torch.LongTensor(length2_list),\n",
    "            torch.LongTensor(label_list)]\n",
    "\n",
    "# Build train, valid and test dataloaders\n",
    "\n",
    "train_dataset = VocabDataset(snli_train, token2id)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(snli_val, token2id)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        # self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(weights).float())\n",
    "\n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    \n",
    "\n",
    "    def forward(self, sent1, length1, sent2, length2):\n",
    "        batch_size, seq_len = sent1.size()\n",
    "\n",
    "        embed = self.embedding(sent1)\n",
    "        embed1 = self.embedding(sent2)\n",
    "\n",
    "        hidden = self.conv1(embed.transpose(1, 2)).transpose(1, 2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = self.conv2(hidden.transpose(1, 2)).transpose(1, 2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden2 = self.conv1(embed1.transpose(1, 2)).transpose(1, 2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, seq_len, hidden2.size(-1))\n",
    "\n",
    "        hidden2 = self.conv2(hidden.transpose(1, 2)).transpose(1, 2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, seq_len, hidden2.size(-1))\n",
    "\n",
    "        hidden = torch.sum(torch.cat([hidden, hidden2], dim=1), dim=1)\n",
    "        logits = self.linear(hidden)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "def test_val_model(loader, model):\n",
    "        \"\"\"\n",
    "        Help function that tests the model's performance on a dataset\n",
    "        @param: loader - data loader for the dataset to test against\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_loss = 0\n",
    "        model.eval()\n",
    "        for sent1, length1, sent2, length2, label in loader:\n",
    "            sent1_batch, length1_batch, sent2_batch, length2_batch, label_batch = sent1, length1, sent2, length2, label\n",
    "            outputs = F.softmax(model(sent1_batch, length1_batch, sent2_batch, length2_batch), dim=1)\n",
    "            predicted = outputs.max(1, keepdim=True)[1]\n",
    "            loss = criterion(outputs, label)\n",
    "            test_loss += loss\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += predicted.eq(label.view_as(predicted)).sum().item()\n",
    "        return (100 * correct / total), test_loss.item()\n",
    "\n",
    "def test_train_model(loader, model):\n",
    "        \"\"\"\n",
    "        Help function that tests the model's performance on a dataset\n",
    "        @param: loader - data loader for the dataset to test against\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        for sent1, length1, sent2, length2, label in loader:\n",
    "            sent1_batch, length1_batch, sent2_batch, length2_batch, label_batch = sent1, length1, sent2, length2, label\n",
    "            outputs = F.softmax(model(sent1_batch, length1_batch, sent2_batch, length2_batch), dim=1)\n",
    "            predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += predicted.eq(label.view_as(predicted)).sum().item()\n",
    "        return (100 * correct / total)\n",
    "\n",
    "\n",
    "#Save Data\n",
    "v_acc = []\n",
    "v_loss= []\n",
    "t_acc = []\n",
    "t_loss= []\n",
    "    \n",
    "    \n",
    "model = CNN(emb_size=300, hidden_size=100, num_layers=2, num_classes=3, vocab_size=len(token2id))\n",
    "test_loss = 0\n",
    "val_loss = 0\n",
    "learning_rate = .0001\n",
    "num_epochs = 5  # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (sent1, length1, sent2, length2, label) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(sent1, length1, sent2, length2)\n",
    "        loss = criterion(outputs, label)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_val_model(val_loader, model)\n",
    "            train_acc = test_train_model(train_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Validation Loss: {} , Test Acc: {}, Test_Loss: {}'\n",
    "                .format(epoch + 1, num_epochs, i + 1, len(train_loader), val_acc[0], val_acc[1] / i, train_acc,\n",
    "                        test_loss / i))\n",
    "            v_acc.append(val_acc[0])\n",
    "            v_loss.append(val_acc[1]/i)\n",
    "            t_acc.append(train_acc)\n",
    "            t_loss.append(test_loss / i)\n",
    "            test_loss = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Results\n",
    "p=np.column_stack((v_acc,v_loss,t_acc,t_loss))\n",
    "np.savetxt(\"CNN-100-weight_deca2y-Hidden.csv\", p, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure out which questions the model got Wrong\n",
    "id2word=dict(zip(sorted((list(token2id.values()))),id2token))\n",
    "\n",
    "def convert_to_word(index_array, dictionary):\n",
    "    words= []\n",
    "    for index in index_array:\n",
    "        word= dictionary[index]\n",
    "        words.append(word)\n",
    "        word_s = ' '.join(words)\n",
    "        \n",
    "    return word_s\n",
    "\n",
    "\n",
    "sent_1=[]\n",
    "sent_2= []\n",
    "hold= []\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0\n",
    "model.eval()\n",
    "for sent1, length1, sent2, length2, label in val_loader:\n",
    "    sent1_batch, length1_batch, sent2_batch, length2_batch, label_batch = sent1, length1, sent2, length2, label\n",
    "    outputs = F.softmax(model(sent1_batch, length1_batch, sent2_batch, length2_batch), dim=1)\n",
    "    predicted = outputs.max(1, keepdim=True)[1]\n",
    "    hold.append(predicted.eq(label.view_as(predicted)))\n",
    "    sent_1.append(sent1)\n",
    "    sent_2.append(sent2)\n",
    "\n",
    "#Correct   \n",
    "(hold[1] == 1).nonzero()[:3]\n",
    "\n",
    "#Incorrect\n",
    "(hold[1] == 0).nonzero()[:3]\n",
    "\n",
    "\n",
    "#Covert to words\n",
    "a=convert_to_word((sent_1[1][1].numpy()).tolist(),id2word)\n",
    "b=convert_to_word((sent_1[1][3].numpy()).tolist(),id2word)\n",
    "c=convert_to_word((sent_1[1][9].numpy()).tolist(),id2word)\n",
    "d=convert_to_word((sent_2[1][0].numpy()).tolist(),id2word)\n",
    "e=convert_to_word((sent_2[1][1].numpy()).tolist(),id2word)\n",
    "f=convert_to_word((sent_2[1][3].numpy()).tolist(),id2word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test of MNLI Ser\n",
    "\n",
    "genres=['fiction', 'government', 'slate', 'telephone', 'travel']\n",
    "labels = ['contradiction', 'entailment', 'neutral']\n",
    "\n",
    "label_dict = dict(zip(labels, range(0, len(labels))))\n",
    "\n",
    "def load_data(file_path, genre):\n",
    "    sent1 = []\n",
    "    sent2 = []\n",
    "    key = []\n",
    "\n",
    "    with open(file_path) as file:\n",
    "        rows = csv.reader(file, delimiter=\"\\t\")\n",
    "        next(rows)  # skips first row\n",
    "        for i,row in enumerate(rows):\n",
    "            if row[3] == genre:\n",
    "                sent1.append(row[0].split())\n",
    "                sent2.append(row[1].split())\n",
    "                key.append(row[2].split())\n",
    "            \n",
    "            \n",
    "\n",
    "        key = [label for labels in key for label in labels]\n",
    "        target = [float(label_dict[item]) for item in key]\n",
    "\n",
    "        return list(zip(sent1, sent2)), target\n",
    "\n",
    "#Data\n",
    "datasets= [load_data(\"mnli_val.tsv\",genre) for genre in genres]\n",
    "\n",
    "val_by_genre =[]\n",
    "\n",
    "for data in datasets:\n",
    "    val_dataset = VocabDataset(data, token2id)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "    val_acc = test_val_model(val_loader, loaded_model)[0]\n",
    "    \n",
    "    val_by_genre.append(val_acc)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results=pd.DataFrame(np.column_stack((genres,val_by_genre)))\n",
    "results.to_csv(\"CNN-MNLI.csv\", header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mnli_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"RNN_best\" + \"model_states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CNN(emb_size=300, hidden_size=100, num_layers=2, num_classes=3, vocab_size=len(token2id))\n",
    "loaded_model.load_state_dict(torch.load('RNN_bestmodel_states'))\n",
    "model.eval()\n",
    "val_acc = test_val_model(val_loader, loaded_model)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
